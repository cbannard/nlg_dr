{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF1yRpTqq_X1"
      },
      "source": [
        "# Training / Fine-tuning a Text Summarisation model\n",
        "\n",
        "We are going to look at model fine-tuning by taking a general text summarisation model and fine-tuning it to perform dialogue summarisation using the SamSum Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULhlBcDPq_X2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -U\n",
        "!pip install transformers -U\n",
        "!pip install datasets\n",
        "!pip install py7zr\n",
        "!pip install tiktoken\n",
        "!pip install sentencepiece\n",
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBdlfSheCGkv"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import pipeline, set_seed\n",
        "from datasets import load_dataset\n",
        "import py7zr\n",
        "import accelerate\n",
        "import pandas as pd\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZOOBYNICGk1"
      },
      "source": [
        "## A dialogue summarisation dataset\n",
        "\n",
        "The SAMSum dataset contains about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English. Linguists were asked to create conversations similar to those they write on a daily basis, reflecting the proportion of topics of their real-life messenger convesations. The style and register are diversified - conversations could be informal, semi-formal or formal, they may contain slang words, emoticons and typos. Then, the conversations were annotated with summaries. It was assumed that summaries should be a concise brief of what people talked about in the conversation in third person. The SAMSum dataset was prepared by Samsung R&D Institute Poland and is distributed for research purposes (non-commercial licence: CC BY-NC-ND 4.0).\n",
        "\n",
        "https://huggingface.co/datasets/Samsung/samsum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5bRIsnNCGk1"
      },
      "outputs": [],
      "source": [
        "dataset_samsum = load_dataset(\"samsum\",trust_remote_code=True)\n",
        "split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\n",
        "\n",
        "print(f\"Split lengths: {split_lengths}\")\n",
        "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
        "print(\"\\nDialogue:\")\n",
        "print(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
        "print(\"\\nSummary:\")\n",
        "print(dataset_samsum[\"test\"][0][\"summary\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCOsSuIZCGk1"
      },
      "source": [
        "### Evaluating PEGASUS on SAMSum\n",
        "\n",
        "We are going to use the SAMSum corpus to fine-tune a general purpose summarisation model for the task of dialogue summarisation. The general purpose model we are going to use is PEGASUS.\n",
        "\n",
        "https://huggingface.co/docs/transformers/en/model_doc/pegasus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHr4G-mBCGkx"
      },
      "source": [
        "<img alt=\"pegasus\" width=\"700\" caption=\"Diagram of PEGASUS architecture (courtesy of Jingqing Zhang et al.)\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter08_pegasus.png?raw=1\" id=\"pegasus\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will load the model and then evaluate its performance on the test section of the SamSUM corpus using the Rouge measure."
      ],
      "metadata": {
        "id": "fpMMTHvCzqkd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf6UqSV3YBZP"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, PegasusTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import tiktoken\n",
        "import sentencepiece\n",
        "#device=\"mps\"\n",
        "device=\"cuda\"\n",
        "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GshMe2cbYBZQ"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "predictions_vanilla=[]\n",
        "for i in range(dataset_samsum[\"test\"].shape[0]):\n",
        "  input_ = tokenizer.batch_encode_plus(dataset_samsum[\"test\"][i:i+1][\"dialogue\"], max_length=1024, pad_to_max_length=True,truncation=True, padding='longest', return_tensors=\"pt\")\n",
        "  input_ids = input_['input_ids']\n",
        "  input_mask = input_['attention_mask']\n",
        "  summaries = model.generate(input_ids=input_ids.to(device),\n",
        "                         attention_mask=input_mask.to(device),\n",
        "                         num_beams=100,\n",
        "                         no_repeat_ngram_size=2,\n",
        "                         early_stopping=True,\n",
        "                         num_return_sequences=1,\n",
        "                         max_length=64)\n",
        "  summaries = tokenizer.batch_decode(summaries, skip_special_tokens=True)\n",
        "  predictions_vanilla.extend([summary.replace(\"<n>\", \"\\n\") for summary in summaries])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix0EoR9uY8Gp"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "references=dataset_samsum[\"test\"][:][\"summary\"]\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge.add(predictions=str(predictions_vanilla), references=str(references))\n",
        "results = rouge.compute()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8QVrtgACGk1"
      },
      "source": [
        "### Fine-Tuning PEGASUS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzpwXQAr2WjH"
      },
      "source": [
        "To fine tune model uncomment the following 5 code blocks and run. Note though that it will take a number of hours to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3pj3VNZCGk1"
      },
      "outputs": [],
      "source": [
        "#def convert_examples_to_features(example_batch):\n",
        "#    input_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024,\n",
        "#                                truncation=True)\n",
        "\n",
        "#    with tokenizer.as_target_tokenizer():\n",
        "#        target_encodings = tokenizer(example_batch[\"summary\"], max_length=128,\n",
        "#                                     truncation=True)\n",
        "\n",
        "#    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
        "#            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
        "#            \"labels\": target_encodings[\"input_ids\"]}\n",
        "\n",
        "#dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features,\n",
        "#                                       batched=True)\n",
        "#columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
        "#dataset_samsum_pt.set_format(type=\"torch\", columns=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE2vmLRgCGk1"
      },
      "outputs": [],
      "source": [
        "#from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
        "\n",
        "#seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "#training_args = TrainingArguments(\n",
        "#    output_dir='pegasus-samsum', num_train_epochs=20, warmup_steps=500,\n",
        "#    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
        "#    weight_decay=0.01, logging_steps=10, push_to_hub=False,\n",
        "#    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,gradient_accumulation_steps=128)\n",
        "\n",
        "#trainer = Trainer(model=model, args=training_args,\n",
        "#                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
        "#                  train_dataset=dataset_samsum_pt[\"train\"],\n",
        "#                  eval_dataset=dataset_samsum_pt[\"validation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efPKAIdJ2WjH"
      },
      "outputs": [],
      "source": [
        "#!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnU37S9eCGk2"
      },
      "outputs": [],
      "source": [
        "#import wandb\n",
        "#from huggingface_hub import notebook_login\n",
        "\n",
        "#notebook_login()\n",
        "#wandb.init(mode=\"disabled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcHfDkIICGk2"
      },
      "outputs": [],
      "source": [
        "# hide_output\n",
        "#torch.cuda.empty_cache()\n",
        "#trainer.train()\n",
        "# To save your fine-tuned model:\n",
        "#trainer.save_model(\"dialogue-summ-model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJDTSdyh2WjH"
      },
      "source": [
        "To load an already fine tuned model uncomment the following cell and run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6UXHg062WjH"
      },
      "outputs": [],
      "source": [
        "model_ckpt=\"transformersbook/pegasus-samsum\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikaKcLsfCGk2"
      },
      "source": [
        "We can evaluate the performance of the fine-tuned model on the test section of the SamSUM corpus as follows:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "predictions=[]\n",
        "for i in range(dataset_samsum[\"test\"].shape[0]):\n",
        "  input_ = tokenizer.batch_encode_plus(dataset_samsum[\"test\"][i:i+1][\"dialogue\"], max_length=1024, pad_to_max_length=True,truncation=True, padding='longest', return_tensors=\"pt\")\n",
        "  input_ids = input_['input_ids']\n",
        "  input_mask = input_['attention_mask']\n",
        "  summaries = model.generate(input_ids=input_ids.to(device),\n",
        "                         attention_mask=input_mask.to(device),\n",
        "                         num_beams=100,\n",
        "                         no_repeat_ngram_size=2,\n",
        "                         early_stopping=True,\n",
        "                         num_return_sequences=1,\n",
        "                         max_length=64)\n",
        "  summaries = tokenizer.batch_decode(summaries, skip_special_tokens=True)\n",
        "  predictions.extend([summary.replace(\"<n>\", \"\\n\") for summary in summaries])"
      ],
      "metadata": {
        "id": "QTTSqbeixzM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "references=dataset_samsum[\"test\"][:][\"summary\"]\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge.add(predictions=str(predictions), references=str(references))\n",
        "results = rouge.compute()\n",
        "print(results)"
      ],
      "metadata": {
        "id": "5zRcG3ZCx3i_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}