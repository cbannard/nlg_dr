{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LELA70502 Directed Reading - Natural Generation with Language Models\n",
        "\n",
        "Generation with language models is also sometimes called auto-regressive generation. It works by selecting and then outputting words from the vocabulary based on their probability given a preceding context - either (at time point 1) a prompt from the user, or (at subsequent time points) the prompt plus the words that have been generated so far.\n",
        "\n",
        "There are however a wide range of ways in which they are chosen. This is what we are going to look at today - different methods of what is know as \"decoding\".\n",
        "\n",
        "While real-world language models generate words based on neural language models (e.g. transformers) we can separate the underlying model from the decoding process and to keep things transparent we are going to start by applying generation methods using a simple n-gram language model.  \n"
      ],
      "metadata": {
        "id": "HurST--5BtN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create bigram probability table"
      ],
      "metadata": {
        "id": "xYCUioPlgCr4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2AqMcpURBL-"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# download from from the internet\n",
        "!wget https://www.gutenberg.org/files/2554/2554-0.txt\n",
        "# read in the file\n",
        "f = open('2554-0.txt')\n",
        "c_and_p = f.read()\n",
        "# Remove the title page etc\n",
        "# convert text to lower case\n",
        "c_and_p=c_and_p[5464:]\n",
        "c_and_p=c_and_p.lower()\n",
        "c_and_p=re.sub('\\n',' ', c_and_p)\n",
        "# Add end of sentence token\n",
        "c_and_p=re.sub(\"\\. \",\" eol \", c_and_p)\n",
        "c_and_p=re.sub('[^a-z ]','', c_and_p)\n",
        "c_and_p=re.sub(' +', ' ',c_and_p)\n",
        "c_and_p=re.split(\" \", c_and_p)\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "total_unigrams = len(c_and_p) - 1\n",
        "bigrams = defaultdict(int)\n",
        "unigrams = defaultdict(int)\n",
        "for i in range(total_unigrams-2):\n",
        "    unigrams[c_and_p[i]] += 1\n",
        "    bigrams[str.join(\" \",c_and_p[i:i+2])] += 1\n",
        "\n",
        "nested_dict = lambda: defaultdict(nested_dict)\n",
        "d = nested_dict()\n",
        "\n",
        "for bg in bigrams:\n",
        "  ug = bg.split()\n",
        "  #print(ug)\n",
        "  d[ug[1]][ug[0]] = np.log(bigrams[bg]/unigrams[ug[0]])\n",
        "\n",
        "lm=pd.DataFrame(d)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm"
      ],
      "metadata": {
        "id": "7UO-zr5PgYwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy Search"
      ],
      "metadata": {
        "id": "JxiWuxPt6uyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define starting word\n",
        "w=\"i\"\n",
        "# Define stopping point - here when an end of line character is output\n",
        "while w != \"eol\":\n",
        "  print(w,end=' ')\n",
        "  # get probabilities for all words following the previous word\n",
        "  s=lm.loc[w]\n",
        "  # sort the probabilities and output the most likely word\n",
        "  w=s.sort_values(ascending=False).index[0]\n"
      ],
      "metadata": {
        "id": "g4Zar9-AfOoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search"
      ],
      "metadata": {
        "id": "0JwyvjCR6W8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify starting word\n",
        "w=\"she\"\n",
        "\n",
        "# Set parameters\n",
        "beam_size=5\n",
        "max_length = 10\n",
        "\n",
        "# initialise variable and data structures\n",
        "length=0\n",
        "candidates = {}\n",
        "candidates[w] = 1\n",
        "old_candidates=pd.Series(candidates)\n",
        "\n",
        "# Define stopping point - here when an end of line character has appeared in all top ranked sentences or a maximum length has been reached\n",
        "while sum([int(\"eol\" in i.split()) for i in old_candidates.index]) < beam_size and length < max_length:\n",
        "  length+=1\n",
        "  candidates={}\n",
        "\n",
        "  # Iterate over list of existing candidates\n",
        "  for ind,(candidate, prob) in enumerate(old_candidates.items()):\n",
        "      # For current candidate find last word and obtain sorted probability distribution over vocab for the next word\n",
        "      cs= candidate.split()\n",
        "      s=lm.loc[cs[-1]]\n",
        "      candidate_words=s.sort_values(ascending=False).index\n",
        "\n",
        "      # Add each candidate word to the end of the current candidate, update the probability of sequence so far to include new word and store in new candidate list\n",
        "      for candidate_word in candidate_words:\n",
        "         c=candidate+\" \"+candidate_word\n",
        "         candidates[c]=prob+lm.loc[cs[-1],candidate_word]\n",
        "  old_candidates=pd.Series(candidates)\n",
        "  # Select top N candidates in line with beam size\n",
        "  old_candidates=old_candidates.sort_values(ascending=False)[0:beam_size]\n",
        "pd.Series(old_candidates).sort_values(ascending=False).index[0]"
      ],
      "metadata": {
        "id": "CIlXSzDrhjKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling"
      ],
      "metadata": {
        "id": "pNs6TJfZ5w47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Specify starting word\n",
        "w=\"he\"\n",
        "# Define stopping point - here when an end of line character is output\n",
        "while w != \"eol\":\n",
        "  print(w,end=' ')\n",
        "  # get probabilities for all words following the previous word\n",
        "  s=lm.loc[w]\n",
        "  s=s.drop(s[np.isnan(s)].index)\n",
        "  # Choose randomly from the probability distribution over next words\n",
        "  w=np.random.choice(list(s.index),1,list(np.exp(s.values)))[0]"
      ],
      "metadata": {
        "id": "V0eggi3r5wNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-k sampling"
      ],
      "metadata": {
        "id": "-XizBvYACXCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "k=10\n",
        "w=\"he\"\n",
        "while w != \"eol\":\n",
        "  print(w,end=' ')\n",
        "  s=lm.loc[w]\n",
        "  s=s.drop(s[np.isnan(s)].index)\n",
        "  s=s.sort_values(ascending=False)[0:k]\n",
        "  newvals=np.exp(s.values)/sum(np.exp(s.values))\n",
        "  w=np.random.choice(list(s.index),1,list(newvals))[0]"
      ],
      "metadata": {
        "id": "8x9FokXrCWjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-p sampling"
      ],
      "metadata": {
        "id": "jpyoOmmMDcuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "w=\"he\"\n",
        "\n",
        "cutoff=0.75\n",
        "\n",
        "def get_p(dist,threshhold):\n",
        "  dist=dist.sort_values(ascending=False)\n",
        "  dist=np.exp(dist)\n",
        "  p=0\n",
        "  cumul=0\n",
        "  while cumul <= threshhold:\n",
        "      cumul = cumul + dist.iloc[p]\n",
        "      p+=1\n",
        "  return p\n",
        "\n",
        "while w != \"eol\":\n",
        "  print(w,end=' ')\n",
        "  s=lm.loc[w]\n",
        "  s=s.drop(s[np.isnan(s)].index)\n",
        "  p=get_p(s,cutoff)\n",
        "  s=s.sort_values(ascending=False)[0:p]\n",
        "  newvals=np.exp(s.values)/sum(np.exp(s.values))\n",
        "  w=np.random.choice(list(s.index),1,list(newvals))[0]\n"
      ],
      "metadata": {
        "id": "jbjPtJL3D-K5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}